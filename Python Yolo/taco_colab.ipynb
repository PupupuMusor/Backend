{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e811f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Установка необходимых библиотек\n",
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Клонирование репозитория TACO\n",
    "!git clone https://github.com/pedropro/TACO.git\n",
    "%cd TACO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Создание скрипта для быстрой загрузки изображений (многопоточный)\n",
    "download_script = '''\n",
    "import os.path\n",
    "import argparse\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "parser = argparse.ArgumentParser(description='')\n",
    "parser.add_argument('--dataset_path', required=False, default= './data/annotations.json', help='Path to annotations')\n",
    "parser.add_argument('--threads', required=False, type=int, default=20, help='Number of parallel downloads')\n",
    "args = parser.parse_args()\n",
    "\n",
    "dataset_dir = os.path.dirname(args.dataset_path)\n",
    "\n",
    "print(\"Starting download...\")\n",
    "\n",
    "# Thread-safe counter for progress\n",
    "counter = 0\n",
    "counter_lock = threading.Lock()\n",
    "nr_images = 0\n",
    "\n",
    "def download_one_image(image):\n",
    "    global counter\n",
    "    \n",
    "    file_name = image[\"file_name\"]\n",
    "    url_original = image[\"flickr_url\"]\n",
    "    url_resized = image[\"flickr_640_url\"]\n",
    "\n",
    "    file_path = os.path.join(dataset_dir, file_name)\n",
    "\n",
    "    # Create subdir if necessary\n",
    "    subdir = os.path.dirname(file_path)\n",
    "    if not os.path.isdir(subdir):\n",
    "        try:\n",
    "            os.makedirs(subdir, exist_ok=True)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        # Load and Save Image\n",
    "        try:\n",
    "            # Try original\n",
    "            response = requests.get(url_original, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                if hasattr(img, \"_getexif\") and img._getexif():\n",
    "                    img.save(file_path, exif=img.info[\"exif\"])\n",
    "                else:\n",
    "                    img.save(file_path)\n",
    "            else:\n",
    "                # Try resized if original fails\n",
    "                response = requests.get(url_resized, timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    img = Image.open(BytesIO(response.content))\n",
    "                    if hasattr(img, \"_getexif\") and img._getexif():\n",
    "                        img.save(file_path, exif=img.info[\"exif\"])\n",
    "                    else:\n",
    "                        img.save(file_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    with counter_lock:\n",
    "        counter += 1\n",
    "        if counter % 50 == 0 or counter == nr_images:\n",
    "            sys.stdout.write(f\"\\rDownloaded {counter}/{nr_images}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "# Load annotations\n",
    "with open(args.dataset_path, 'r') as f:\n",
    "    annotations = json.loads(f.read())\n",
    "    images = annotations['images']\n",
    "    nr_images = len(images)\n",
    "\n",
    "    print(f\"Downloading {nr_images} images using {args.threads} threads...\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:\n",
    "        executor.map(download_one_image, images)\n",
    "\n",
    "    print('\\nFinished')\n",
    "'''\n",
    "\n",
    "with open('download_fast.py', 'w') as f:\n",
    "    f.write(download_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de16d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Запуск загрузки изображений\n",
    "!python download_fast.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Создание карты классов (Basic 5: Plastic, Paper, Glass, Metal, Organic, Other)\n",
    "map_basic_content = \"\"\"Aerosol,Metal\n",
    "Aluminium foil,Metal\n",
    "Battery,Other\n",
    "Aluminium blister pack,Other\n",
    "Carded blister pack,Other\n",
    "Clear plastic bottle,Plastic\n",
    "Glass bottle,Glass\n",
    "Other plastic bottle,Plastic\n",
    "Plastic bottle cap,Plastic\n",
    "Metal bottle cap,Metal\n",
    "Broken glass,Glass\n",
    "Drink can,Metal\n",
    "Food Can,Metal\n",
    "Corrugated carton,Paper\n",
    "Drink carton,Paper\n",
    "Egg carton,Paper\n",
    "Meal carton,Paper\n",
    "Other carton,Paper\n",
    "Paper cup,Paper\n",
    "Disposable plastic cup,Plastic\n",
    "Foam cup,Plastic\n",
    "Glass cup,Glass\n",
    "Other plastic cup,Plastic\n",
    "Food waste,Organic\n",
    "Plastic lid,Plastic\n",
    "Metal lid,Metal\n",
    "Magazine paper,Paper\n",
    "Tissues,Paper\n",
    "Wrapping paper,Paper\n",
    "Normal paper,Paper\n",
    "Paper bag,Paper\n",
    "Plastified paper bag,Paper\n",
    "Pizza box,Paper\n",
    "Garbage bag,Plastic\n",
    "Single-use carrier bag,Plastic\n",
    "Polypropylene bag,Plastic\n",
    "Produce bag,Plastic\n",
    "Cereal bag,Plastic\n",
    "Bread bag,Plastic\n",
    "Plastic film,Plastic\n",
    "Crisp packet,Plastic\n",
    "Other plastic wrapper,Plastic\n",
    "Retort pouch,Plastic\n",
    "Spread tub,Plastic\n",
    "Tupperware,Plastic\n",
    "Disposable food container,Plastic\n",
    "Foam food container,Plastic\n",
    "Other plastic container,Plastic\n",
    "Plastic glooves,Plastic\n",
    "Plastic utensils,Plastic\n",
    "Pop tab,Metal\n",
    "Rope & strings,Other\n",
    "Scrap metal,Metal\n",
    "Shoe,Other\n",
    "Six pack rings,Plastic\n",
    "Squeezable tube,Plastic\n",
    "Plastic straw,Plastic\n",
    "Paper straw,Paper\n",
    "Styrofoam piece,Plastic\n",
    "Toilet tube,Paper\n",
    "Unlabeled litter,Other\n",
    "Glass jar,Glass\n",
    "Other plastic,Plastic\n",
    "Cigarette,Other\n",
    "\"\"\"\n",
    "\n",
    "with open('detector/taco_config/map_basic.csv', 'w') as f:\n",
    "    f.write(map_basic_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9732e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Создание скрипта конвертации в YOLO\n",
    "convert_script = '''\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import random\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "ANNOTATIONS_FILE = 'data/annotations.json'\n",
    "IMAGES_DIR = 'data'\n",
    "OUTPUT_DIR = 'yolo_dataset'\n",
    "MAP_FILE = 'detector/taco_config/map_basic.csv'\n",
    "\n",
    "def load_class_map(map_file):\n",
    "    mapping = {}\n",
    "    new_classes = set()\n",
    "    with open(map_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) >= 2:\n",
    "                mapping[row[0]] = row[1]\n",
    "                new_classes.add(row[1])\n",
    "    \n",
    "    # Sort classes to ensure consistent ordering\n",
    "    sorted_classes = sorted(list(new_classes))\n",
    "    class_to_id = {name: i for i, name in enumerate(sorted_classes)}\n",
    "    \n",
    "    return mapping, class_to_id, sorted_classes\n",
    "\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = box[0] + box[2] / 2.0\n",
    "    y = box[1] + box[3] / 2.0\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)\n",
    "\n",
    "def main():\n",
    "    # 1. Load Class Mapping\n",
    "    print(\"Loading class mapping...\")\n",
    "    cat_name_map, new_class_to_id, class_names = load_class_map(MAP_FILE)\n",
    "    print(f\"Found {len(class_names)} target classes: {class_names}\")\n",
    "\n",
    "    # 2. Load Annotations\n",
    "    print(\"Loading annotations...\")\n",
    "    with open(ANNOTATIONS_FILE, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create ID to Name mapping for original categories\n",
    "    original_cat_id_to_name = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "\n",
    "    # 3. Prepare Directories\n",
    "    for split in ['train', 'val']:\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_DIR, 'labels', split), exist_ok=True)\n",
    "\n",
    "    # 4. Process Images\n",
    "    images = data['images']\n",
    "    annotations = data['annotations']\n",
    "    \n",
    "    # Group annotations by image_id\n",
    "    img_anns = {}\n",
    "    for ann in annotations:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in img_anns:\n",
    "            img_anns[img_id] = []\n",
    "        img_anns[img_id].append(ann)\n",
    "\n",
    "    # Split dataset\n",
    "    random.seed(42)\n",
    "    random.shuffle(images)\n",
    "    split_idx = int(len(images) * 0.9)\n",
    "    train_images = images[:split_idx]\n",
    "    val_images = images[split_idx:]\n",
    "\n",
    "    print(\"Converting data...\")\n",
    "    \n",
    "    for split, split_images in [('train', train_images), ('val', val_images)]:\n",
    "        for img in tqdm(split_images, desc=f\"Processing {split}\"):\n",
    "            img_id = img['id']\n",
    "            file_name = img['file_name']\n",
    "            \n",
    "            # Source path\n",
    "            src_path = os.path.join(IMAGES_DIR, file_name)\n",
    "            if not os.path.exists(src_path):\n",
    "                continue\n",
    "\n",
    "            # Destination path\n",
    "            # Flatten directory structure for YOLO (batch_1/001.jpg -> batch_1_001.jpg)\n",
    "            flat_name = file_name.replace('/', '_').replace('\\\\', '_')\n",
    "            dst_img_path = os.path.join(OUTPUT_DIR, 'images', split, flat_name)\n",
    "            dst_label_path = os.path.join(OUTPUT_DIR, 'labels', split, flat_name.replace('.jpg', '.txt'))\n",
    "\n",
    "            # Copy image\n",
    "            shutil.copy(src_path, dst_img_path)\n",
    "\n",
    "            # Create label file\n",
    "            with open(dst_label_path, 'w') as out_f:\n",
    "                if img_id in img_anns:\n",
    "                    for ann in img_anns[img_id]:\n",
    "                        original_cat_id = ann['category_id']\n",
    "                        original_name = original_cat_id_to_name.get(original_cat_id)\n",
    "                        \n",
    "                        # Map to new class\n",
    "                        if original_name in cat_name_map:\n",
    "                            new_class_name = cat_name_map[original_name]\n",
    "                            new_class_id = new_class_to_id[new_class_name]\n",
    "                            \n",
    "                            # Convert bbox\n",
    "                            bbox = convert_bbox((img['width'], img['height']), ann['bbox'])\n",
    "                            \n",
    "                            out_f.write(f\"{new_class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\\\n\")\n",
    "\n",
    "    # 5. Create data.yaml\n",
    "    yaml_content = {\n",
    "        'path': os.path.abspath(OUTPUT_DIR),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'names': {i: name for i, name in enumerate(class_names)}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(OUTPUT_DIR, 'data.yaml'), 'w') as f:\n",
    "        yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "    print(f\"Conversion complete! Dataset saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "with open('convert_to_yolo.py', 'w') as f:\n",
    "    f.write(convert_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f660ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Запуск конвертации\n",
    "!python convert_to_yolo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Обучение модели YOLOv8\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Автоматическое определение устройства (GPU или CPU)\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cpu':\n",
    "    print(\"⚠️ ВНИМАНИЕ: GPU не найден! Обучение будет очень медленным.\")\n",
    "    print(\"Если вы в Google Colab: Зайдите в 'Среда выполнения' -> 'Сменить среду выполнения' -> выберите 'T4 GPU'.\")\n",
    "\n",
    "# Используем модель Small (s) для баланса скорости и точности\n",
    "model = YOLO('yolov8s.pt')\n",
    "\n",
    "results = model.train(\n",
    "    data='yolo_dataset/data.yaml',\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='taco_yolov8',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb52226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Архивация результатов для скачивания\n",
    "!zip -r taco_yolov8_results.zip runs/detect/taco_yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Скачивание архива (если не скачалось автоматически, найдите файл в панели слева)\n",
    "from google.colab import files\n",
    "files.download('taco_yolov8_results.zip')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
